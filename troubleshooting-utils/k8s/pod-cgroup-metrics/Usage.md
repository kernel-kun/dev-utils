# Resource Monitoring Scripts Usage Guide

This guide provides step-by-step instructions for using the resource monitoring scripts to track CPU, memory, and system metrics for Kubernetes pods running GitHub Actions runners.

## 📁 File Overview

- **`monitor_resources.sh`** - Main monitoring script that collects real-time metrics
- **`resource_analyzer.py`** - Python script for analyzing collected data and generating visualizations
- **`debug_cgroups.py`** - Debug utility to inspect cgroup file structure
- **`requirements.txt`** - Python dependencies
- **`ResourceMonitoringGuide.md`** - Comprehensive documentation of all metrics

## 🚀 Quick Start

### 1. Prerequisites

```bash
# Install Python dependencies
pip install -r requirements.txt

# Make scripts executable (Linux/macOS)
chmod +x monitor_resources.sh
chmod +x debug_cgroups.py
```

### 2. Basic Monitoring

Monitor a GitHub Actions runner pod for 5 minutes:

```bash
./monitor_resources.sh -n github-actions -p runner-abc123 -d 5 -i 2
```

### 3. Analyze Results

```bash
python3 resource_analyzer.py monitoring_output/resource_metrics_20231201_120000.csv
```

## 📊 Detailed Usage

### Monitor Resources Script

#### Syntax
```bash
./monitor_resources.sh -n NAMESPACE -p POD_NAME [OPTIONS]
```

#### Required Parameters
- `-n, --namespace NAMESPACE` - Kubernetes namespace where the pod is running
- `-p, --pod POD_NAME` - Name of the pod to monitor

#### Optional Parameters
- `-d, --duration MINUTES` - Monitoring duration in minutes (default: 60)
- `-i, --interval SECONDS` - Data collection interval in seconds (default: 2)
- `-o, --output-dir DIR` - Output directory for CSV files (default: ./monitoring_output)
- `-h, --help` - Show help message

#### Examples

**Monitor for 30 minutes with 5-second intervals:**
```bash
./monitor_resources.sh -n my-ns -p ss-prod-dev-runner-xyz -d 30 -i 5
```

**Custom output directory:**
```bash
./monitor_resources.sh -n github-actions -p runner-123 -d 10 -o /tmp/monitoring
```

#### Output

The script creates CSV files in the format:
```
monitoring_output/resource_metrics_YYYYMMDD_HHMMSS.csv
```

Each CSV contains 85+ metrics including:
- CPU usage, user/system time, throttling events
- Memory current/max, RSS, cache, swap usage
- Process counts, system load, disk usage
- Network I/O statistics

### Resource Analyzer Script

#### Syntax
```bash
python3 resource_analyzer.py CSV_FILE [OPTIONS]
```

#### Parameters
- `CSV_FILE` - Path to the CSV file generated by monitor_resources.sh
- `-o, --output OUTPUT_DIR` - Directory to save visualization images (default: current directory)

#### Examples

**Basic analysis:**
```bash
python3 resource_analyzer.py monitoring_output/resource_metrics_20231201_120000.csv
```

**Save to custom directory:**
```bash
python3 resource_analyzer.py data.csv -o ./analysis_results/
```

#### Generated Outputs

1. **Console Summary** - Text-based resource statistics and insights
2. **resource_overview.png** - 4-panel visualization showing:
   - CPU Usage Over Time (with reference lines for 1 CPU and 2 CPU)
   - Memory Usage vs Limits (current usage vs 8GB limits)
   - CPU Throttling Events (percentage of throttled periods)
   - Process Count & System Load

#### Understanding the Visualizations

**CPU Usage Graph:**
- Shows millicores (1000m = 1 CPU core)
- Reference lines at 1000m and 2000m for easy comparison
- Tracks both runner and dind containers

**Memory Usage Graph:**
- Shows memory in GB with relevant limit lines
- Displays current memory consumption patterns
- Helps identify memory leaks or excessive usage

**CPU Throttling Graph:**
- Shows percentage of CPU periods that were throttled
- High values indicate CPU resource constraints
- Zero values suggest adequate CPU allocation

**Process Count & System Load:**
- Process count for each container
- System load average (1.0 = fully utilized single core)
- Helps identify process explosion or system stress

### Debug Utilities

#### Debug Cgroups Script

Inspect the cgroup file structure and contents:

```bash
./debug_cgroups.py -n NAMESPACE -p POD_NAME
```

This shows:
- Available cgroup subsystems
- File paths and permissions
- Sample file contents
- Cgroup version detection

## 🎯 Use Cases

### 1. Performance Troubleshooting

Monitor a slow-running job:
```bash
# Start monitoring
./monitor_resources.sh -n ci-cd -p slow-runner-456 -d 60 -i 1

# Analyze results
python3 resource_analyzer.py monitoring_output/resource_metrics_*.csv
```

Look for:
- High CPU usage (>2000m sustained)
- Memory approaching limits (>7GB)
- High system load (>2.0)
- Process count spikes

### 2. Resource Optimization

Compare resource usage across different job types:
```bash
# Monitor lightweight job
./monitor_resources.sh -n ci -p light-job-123 -d 15

# Monitor heavy job  
./monitor_resources.sh -n ci -p heavy-job-456 -d 15

# Compare results
python3 resource_analyzer.py monitoring_output/resource_metrics_light_*.csv
python3 resource_analyzer.py monitoring_output/resource_metrics_heavy_*.csv
```

### 3. Noisy Neighbor Detection

Monitor during suspected resource contention:
```bash
# Extended monitoring during peak hours
./monitor_resources.sh -n production -p runner-abc -d 120 -i 2

# Look for correlation between containers
python3 resource_analyzer.py monitoring_output/resource_metrics_*.csv
```

Signs of noisy neighbors:
- Synchronized resource spikes between containers
- High system load with normal CPU usage
- Memory pressure events
- Increased throttling events

### 4. Capacity Planning

Establish baseline resource usage:
```bash
# Monitor typical workloads
./monitor_resources.sh -n ci -p baseline-runner -d 180 -i 5

# Generate capacity recommendations
python3 resource_analyzer.py monitoring_output/resource_metrics_*.csv
```

Use the data to:
- Set appropriate resource requests/limits
- Plan cluster capacity
- Optimize pod scheduling

## 🔧 Troubleshooting

### Common Issues

**1. Permission Denied**
```bash
chmod +x monitor_resources.sh debug_cgroups.py
```

**2. Python Module Not Found**
```bash
pip install -r requirements.txt
```

**3. Kubectl Access Issues**
```bash
# Verify cluster access
kubectl get pods -n YOUR_NAMESPACE

# Check pod exists
kubectl get pod POD_NAME -n NAMESPACE
```

**4. No Data Collected**
```bash
# Run debug mode
./monitor_resources.sh -n NAMESPACE -p POD_NAME
```

**5. Empty Visualizations**
- Check CSV file has data beyond the header
- Verify monitoring ran for at least 2 intervals
- Ensure containers were active during monitoring

### Interpreting Zero Values

**Expected Zeros (cgroups v1):**
- All PSI (Pressure Stall Information) metrics
- CPU throttling (if no limits set)
- Swap usage (if swap disabled)

**Unexpected Zeros:**
- CPU usage (indicates measurement issue)
- Memory current (indicates collection failure)
- Process count (suggests container problems)

## 📈 Best Practices

### Monitoring Duration
- **Quick check**: 5-10 minutes
- **Performance analysis**: 30-60 minutes  
- **Baseline establishment**: 2-4 hours
- **Noisy neighbor detection**: 1-2 hours during peak

### Interval Selection
- **High-frequency monitoring**: 1-2 seconds (short duration only)
- **Standard monitoring**: 5-10 seconds
- **Long-term monitoring**: 30-60 seconds

### Resource Thresholds
- **CPU**: >80% sustained indicates need for more cores
- **Memory**: >90% suggests increasing memory limits
- **System Load**: >2.0 indicates potential scheduling issues
- **Throttling**: >5% suggests CPU limits too low

### Data Management
```bash
# Clean old monitoring data
find monitoring_output/ -name "*.csv" -mtime +7 -delete

# Archive important analysis results
tar -czf analysis_$(date +%Y%m%d).tar.gz *.png monitoring_output/
```

## 📚 Additional Resources

- **ResourceMonitoringGuide.md** - Complete parameter documentation and calculations
- **GitHub Actions Documentation** - Runner configuration best practices
- **Kubernetes Resource Management** - Official capacity planning guides

For detailed metric explanations and advanced use cases, refer to the [ResourceMonitoringGuide.md](ResourceMonitoringGuide.md) file.
